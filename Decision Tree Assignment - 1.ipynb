{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. Describe the decision tree classifier algorithm and how it works to make predictions.\n",
    "# A decision tree classifier splits the data into subsets based on feature values.\n",
    "# It uses a tree structure where each internal node represents a feature, each branch represents a decision, and each leaf node represents a class label.\n",
    "# Predictions are made by traversing the tree from the root node to a leaf node based on feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. Provide a step-by-step explanation of the mathematical intuition behind decision tree classification.\n",
    "# 1. Calculate Entropy: Measure the impurity or uncertainty in the data.\n",
    "# Formula: Entropy = -Î£ p(x) * log2(p(x))\n",
    "# 2. Calculate Information Gain: Measure the reduction in entropy after splitting the data based on a feature.\n",
    "# Formula: Information Gain = Entropy(parent) - Weighted Sum(Entropy(children))\n",
    "# 3. Split the data using the feature that provides the highest information gain.\n",
    "# 4. Repeat the process recursively until the stopping criteria (e.g., minimum samples per leaf) are met.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. Explain how a decision tree classifier can be used to solve a binary classification problem.\n",
    "# In binary classification, the decision tree splits the data into two groups at each node.\n",
    "# It assigns labels (0 or 1) based on whether the data satisfies the condition at each split.\n",
    "# The process continues until all samples in a node belong to the same class or the stopping condition is met.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. Discuss the geometric intuition behind decision tree classification and how it can be used to make predictions.\n",
    "# Decision trees partition the feature space into regions using axis-aligned splits.\n",
    "# Each split creates a boundary, dividing the space into rectangles or hyperrectangles.\n",
    "# Predictions are made by determining which region a sample falls into and assigning the corresponding class label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. Define the confusion matrix and describe how it can be used to evaluate the performance of a classification model.\n",
    "# A confusion matrix is a table that summarizes prediction results.\n",
    "# It contains four values: True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN).\n",
    "# It helps evaluate metrics such as accuracy, precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[2 0]\n",
      " [1 3]]\n",
      "Precision: 1.0\n",
      "Recall: 0.75\n",
      "F1 Score: 0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "# Q6. Provide an example of a confusion matrix and explain how precision, recall, and F1 score can be calculated from it.\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "# Example data\n",
    "y_true = [1, 0, 1, 1, 0, 1]\n",
    "y_pred = [1, 0, 1, 0, 0, 1]\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print('Confusion Matrix:\\n', cm)\n",
    "\n",
    "# Calculate metrics\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1 Score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. Discuss the importance of choosing an appropriate evaluation metric for a classification problem and explain how this can be done.\n",
    "# Choosing the right metric depends on the problem requirements and class distribution.\n",
    "# For balanced datasets, accuracy may be sufficient.\n",
    "# For imbalanced datasets, precision, recall, or F1-score is more appropriate.\n",
    "# Metrics should align with the business goals and consequences of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8. Provide an example of a classification problem where precision is the most important metric, and explain why.\n",
    "# Example: Spam email detection.\n",
    "# Precision is critical because predicting a non-spam email as spam can result in important messages being missed.\n",
    "\n",
    "# Q9. Provide an example of a classification problem where recall is the most important metric, and explain why.\n",
    "# Example: Medical diagnosis for cancer detection.\n",
    "# Recall is crucial as missing a positive case can have severe consequences for the patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
